{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f7bdd4c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThis module is responsible for reading outage email files from a folder\\nand using an LLM (like GPT-4) to extract structured information\\nsuch as partner name, outage duration, cause, etc.\\n\\nThe output is returned as a pandas DataFrame that can later be loaded\\ninto DuckDB for analytics and reporting.\\n\\nHigh-level workflow:\\n1. Read all `.txt` email files from a folder.\\n2. For each email, send the text to the LLM.\\n3. Ask the LLM to return JSON data with fixed fields.\\n4. Convert all outputs into a clean DataFrame.\\n\\n'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "This module is responsible for reading outage email files from a folder\n",
    "and using an LLM (like GPT-4) to extract structured information\n",
    "such as partner name, outage duration, cause, etc.\n",
    "\n",
    "The output is returned as a pandas DataFrame that can later be loaded\n",
    "into DuckDB for analytics and reporting.\n",
    "\n",
    "High-level workflow:\n",
    "1. Read all `.txt` email files from a folder.\n",
    "2. For each email, send the text to the LLM.\n",
    "3. Ask the LLM to return JSON data with fixed fields.\n",
    "4. Convert all outputs into a clean DataFrame.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "29cd1eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from langchain_openai import ChatOpenAI  # LangChain's OpenAI chat model interface\n",
    "from dotenv import load_dotenv \n",
    "from pathlib import Path\n",
    "load_dotenv(override=True)  # take environment variables from .env file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "66c78c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\AI Utils\\Partner Performance Report\\Global_Hitec_Outage_Emails GLOBAL_HITEC_OUTAGE_JSON\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Read environment variables\n",
    "# -------------------------------\n",
    "HITEC_EMAIL_DIR = os.getenv(\"GLOBAL_HITEC_EMAIL_DIR\")\n",
    "HITEC_EMAIL_JSON = os.getenv(\"GLOBAL_HITEC_EMAIL_JSON\")\n",
    "print(HITEC_EMAIL_DIR, HITEC_EMAIL_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b3abdd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# STEP 1: Create a helper function to initialize the LLM model\n",
    "# -------------------------------------------------------------------\n",
    "def get_llm(model_name=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    This function initializes the Large Language Model (LLM).\n",
    "    In this case, we‚Äôre using the GPT-4 model provided through LangChain.\n",
    "\n",
    "    - model_name: defines which OpenAI model to use.\n",
    "    - temperature: controls randomness (0 = deterministic answers).\n",
    "\n",
    "    Returns:\n",
    "        llm object ‚Üí can be used to call llm.predict(prompt)\n",
    "    \"\"\"\n",
    "    \n",
    "    return ChatOpenAI(model_name=model_name, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e4cd6a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# STEP 2: Define the LLM prompt template\n",
    "# -------------------------------------------------------------------\n",
    "PARSER_PROMPT = \"\"\"\n",
    "You are an expert system that parses structured outage summary emails.\n",
    "Your task is to extract outage-related details from the email below.\n",
    "\n",
    "Extract the following fields and return ONLY valid JSON (no explanations):\n",
    "\n",
    "- partner_name\n",
    "- outage_type\n",
    "- issue_details\n",
    "- current_status\n",
    "- business_impact\n",
    "- manual_processing\n",
    "- root_cause_available\n",
    "- outage_start_time (ISO 8601 format)\n",
    "- outage_end_time (ISO 8601 format)\n",
    "- duration_hours (numeric)\n",
    "- resolution_details\n",
    "- email_subject\n",
    "- email_date\n",
    "\n",
    "Input Email:\n",
    "-------------------\n",
    "{email_text}\n",
    "-------------------\n",
    "\n",
    "Rules:\n",
    "- Return only valid JSON.\n",
    "- Convert all dates to ISO 8601 format (e.g., 2025-11-05T07:48:00-08:00).\n",
    "- If data is missing, put null.\n",
    "- If start and end times exist, calculate duration_hours.\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "be7dcf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# STEP 3: Create a function to parse ONE email using LLM\n",
    "# -------------------------------------------------------------------\n",
    "from urllib import response\n",
    "\n",
    "\n",
    "def parse_outage_email(llm, email_text: str):\n",
    "    \"\"\"\n",
    "    Sends one email‚Äôs text to the LLM and returns a parsed JSON object.\n",
    "\n",
    "    Parameters:\n",
    "        llm        ‚Üí The initialized LLM model object (from get_llm()).\n",
    "        email_text ‚Üí The raw text of one outage email.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing all parsed fields + a timestamp.\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare the LLM prompt by inserting the email text into the template\n",
    "    prompt = PARSER_PROMPT.format(email_text=email_text)\n",
    "\n",
    "    try:\n",
    "        # Send the prompt to the LLM and capture its response\n",
    "        response = llm.invoke(prompt)\n",
    "        raw_response = response.content.strip()\n",
    "\n",
    "        # The LLM might include extra text before/after JSON, so extract clean JSON only\n",
    "        start = raw_response.find(\"{\")\n",
    "        end = raw_response.rfind(\"}\")\n",
    "        if start == -1 or end == -1:\n",
    "            raise ValueError(\"No valid JSON object found in LLM response.\")\n",
    "\n",
    "        # Extract JSON substring\n",
    "        json_str = raw_response[start:end+1]\n",
    "\n",
    "        # Convert JSON string into Python dictionary\n",
    "        parsed = json.loads(json_str)\n",
    "\n",
    "    except Exception as e:\n",
    "        # If something goes wrong (bad format, LLM failure, etc.)\n",
    "        # we capture the error but still return a record (so processing continues)\n",
    "        parsed = {\n",
    "            \"error\": str(e),\n",
    "            \"raw_text\": email_text[:2000]  # keep partial text for debugging\n",
    "        }\n",
    "\n",
    "    # Add a timestamp to track when this email was parsed\n",
    "    parsed[\"parsed_at\"] = datetime.utcnow().isoformat()\n",
    "\n",
    "    # Return a Python dictionary of the parsed data\n",
    "    return parsed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f56f2d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------------------------\n",
    "# STEP 4: Parse ALL emails from a folder\n",
    "# -------------------------------------------------------------------\n",
    "def parse_emails_from_folder(folder_path: str, llm):\n",
    "    \"\"\"\n",
    "    Reads all text (.txt) email files from the given folder,\n",
    "    uses the LLM to parse each one, and returns a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        folder_path ‚Üí path to folder containing email files\n",
    "        llm         ‚Üí LLM object returned by get_llm()\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame containing one row per email with structured fields\n",
    "    \"\"\"\n",
    "    #Create full directory path\n",
    "    full_folder_path = Path(folder_path)\n",
    "    print(\"full folder path *********\" + str(full_folder_path))\n",
    "    # Create an empty list to hold results\n",
    "    records = []\n",
    "\n",
    "    # Loop through all files in the folder\n",
    "    for file in os.listdir(full_folder_path):\n",
    "        # Skip non-text files\n",
    "        if not file.endswith(\".txt\"):\n",
    "            continue\n",
    "\n",
    "        # Full file path\n",
    "        full_path = os.path.join(full_folder_path, file)\n",
    "        print(\"full file path *********\" + str(full_path))\n",
    "\n",
    "        # Read the email content from the file\n",
    "        with open(full_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            email_text = f.read()\n",
    "\n",
    "        print(f\"üîç Parsing email file: {file} ...\")\n",
    "\n",
    "        # Parse one email using the LLM\n",
    "        parsed = parse_outage_email(llm, email_text)\n",
    "\n",
    "        # Keep track of the original file name for traceability\n",
    "        parsed[\"source_file\"] = file\n",
    "\n",
    "        # Add parsed dictionary to list\n",
    "        records.append(parsed)\n",
    "\n",
    "    # Convert list of dictionaries into a pandas DataFrame\n",
    "    df = pd.DataFrame(records)\n",
    "\n",
    "    print(f\"‚úÖ Parsed {len(df)} emails successfully.\")\n",
    "    print(df.head())\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8fdbeab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "storage.py\n",
    "-----------\n",
    "This module handles storing the parsed outage email data\n",
    "into a DuckDB database.\n",
    "\n",
    "DuckDB is a lightweight, high-speed, SQL-based analytical database\n",
    "that runs in a single file (like SQLite, but optimized for analytics).\n",
    "\n",
    "The workflow:\n",
    "1. Initialize a DuckDB connection and create a table (if not exists).\n",
    "2. Insert parsed data (from a pandas DataFrame) into that table.\n",
    "\"\"\"\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# STEP 1: Define the database schema (table structure)\n",
    "# -------------------------------------------------------------------\n",
    "DB_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS outages (\n",
    "    partner_name VARCHAR,\n",
    "    outage_type VARCHAR,\n",
    "    issue_details VARCHAR,\n",
    "    current_status VARCHAR,\n",
    "    business_impact VARCHAR,\n",
    "    manual_processing VARCHAR,\n",
    "    root_cause_available VARCHAR,\n",
    "    outage_start_time TIMESTAMP,\n",
    "    outage_end_time TIMESTAMP,\n",
    "    duration_hours DOUBLE,\n",
    "    resolution_details VARCHAR,\n",
    "    email_subject VARCHAR,\n",
    "    email_date DATE,\n",
    "    parsed_at TIMESTAMP,\n",
    "    source_file VARCHAR\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# STEP 2: Function to initialize DuckDB\n",
    "# -------------------------------------------------------------------\n",
    "def init_db(in_memory=True):\n",
    "    \"\"\"\n",
    "    Initializes the DuckDB database.\n",
    "\n",
    "    Parameters:\n",
    "        in_memory ‚Üí if True, creates a temporary DB (for testing);\n",
    "                     if False, saves data to a local file \"outages.duckdb\".\n",
    "\n",
    "    Returns:\n",
    "        Connection object ‚Üí used to run SQL commands.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a connection to the database\n",
    "    if in_memory:\n",
    "        con = duckdb.connect(database=\":memory:\")\n",
    "        print(\"üß† Using in-memory DuckDB (data will be lost after shutdown).\")\n",
    "    else:\n",
    "        con = duckdb.connect(database=\"outages.duckdb\")\n",
    "        print(\"üíæ Using persistent DuckDB file (outages.duckdb).\")\n",
    "\n",
    "    # Create table if it doesn‚Äôt exist yet\n",
    "    con.execute(DB_SCHEMA)\n",
    "    return con\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# STEP 3: Function to load a DataFrame into DuckDB\n",
    "# -------------------------------------------------------------------\n",
    "def load_dataframe_to_duckdb(con, df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Inserts the parsed outage email data (from pandas DataFrame)\n",
    "    into the 'outages' table in DuckDB.\n",
    "\n",
    "    Parameters:\n",
    "        con ‚Üí active DuckDB connection\n",
    "        df  ‚Üí pandas DataFrame containing parsed email data\n",
    "    \"\"\"\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"‚ö†Ô∏è No records found ‚Äî nothing to insert.\")\n",
    "        return\n",
    "\n",
    "    # Register the DataFrame as a temporary table in DuckDB\n",
    "    con.register(\"df_temp\", df)\n",
    "\n",
    "    # Insert data into the main outages table using SQL\n",
    "    con.execute(\"\"\"\n",
    "        INSERT INTO outages\n",
    "        SELECT \n",
    "            partner_name,\n",
    "            outage_type,\n",
    "            issue_details,\n",
    "            current_status,\n",
    "            business_impact,\n",
    "            manual_processing,\n",
    "            root_cause_available,\n",
    "            outage_start_time,\n",
    "            outage_end_time,\n",
    "            duration_hours,\n",
    "            resolution_details,\n",
    "            email_subject,\n",
    "            email_date,\n",
    "            parsed_at,\n",
    "            source_file\n",
    "        FROM df_temp\n",
    "    \"\"\")\n",
    "\n",
    "    # Unregister temporary table\n",
    "    con.unregister(\"df_temp\")\n",
    "\n",
    "    print(f\"‚úÖ {len(df)} records loaded into DuckDB successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ede5793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm_tools.py\n",
    "\n",
    "import json\n",
    "import io\n",
    "import base64\n",
    "import os\n",
    "import tempfile\n",
    "import matplotlib.pyplot as plt\n",
    "from email.message import EmailMessage\n",
    "import smtplib\n",
    "from pptx import Presentation\n",
    "from pptx.util import Inches\n",
    "from langchain_core.tools import Tool\n",
    "from langchain.tools import tool\n",
    "\n",
    "\n",
    "def create_llm(model_name=\"gpt-4o-mini\"):\n",
    "    return ChatOpenAI(model_name=model_name, temperature=0)\n",
    "\n",
    "def run_sql_tool_factory(con):\n",
    "    print(\"Creating run_sql tool\")\n",
    "    def run_sql(sql):\n",
    "        print(f\"Executing SQL: {sql}\")\n",
    "        forbidden = [\"INSERT\",\"UPDATE\",\"DELETE\",\"DROP\",\"ALTER\",\"CREATE\",\"--\",\";\"]\n",
    "        if any(k in sql.upper() for k in forbidden):\n",
    "            return json.dumps({\"error\": \"Forbidden SQL command\"})\n",
    "        try:\n",
    "            df = con.execute(sql).df()\n",
    "            # convert datetimes\n",
    "            for c in df.columns:\n",
    "                if \"datetime\" in str(df[c].dtype):\n",
    "                    df[c] = df[c].astype(str)\n",
    "            return df.to_json(orient=\"records\")\n",
    "        except Exception as e:\n",
    "            return json.dumps({\"error\": str(e)})\n",
    "    return run_sql\n",
    "\n",
    "def create_chart_tool(data_json, chart_type=\"bar\", x=None, y=None):\n",
    "    data = json.loads(data_json)\n",
    "    if not data:\n",
    "        raise ValueError(\"Empty data\")\n",
    "\n",
    "    cols = list(data[0].keys())\n",
    "    x = x or cols[0]\n",
    "    y = y or cols[1]\n",
    "\n",
    "    xs = [str(r[x]) for r in data]\n",
    "    ys = [r[y] for r in data]\n",
    "\n",
    "    plt.figure(figsize=(8,4))\n",
    "    if chart_type == \"line\":\n",
    "        plt.plot(xs, ys, marker=\"o\")\n",
    "    else:\n",
    "        plt.bar(xs, ys)\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format=\"png\")\n",
    "    buf.seek(0)\n",
    "    plt.close()\n",
    "\n",
    "    return \"data:image/png;base64,\" + base64.b64encode(buf.read()).decode()\n",
    "\n",
    "def create_ppt_tool_factory(con):\n",
    "    def create_ppt(design_json):\n",
    "        design = json.loads(design_json)\n",
    "        prs = Presentation()\n",
    "\n",
    "        for slide_spec in design.get(\"slides\", []):\n",
    "            layout = prs.slide_layouts[0] if slide_spec[\"type\"] == \"title\" else prs.slide_layouts[1]\n",
    "            slide = prs.slides.add_slide(layout)\n",
    "            slide.shapes.title.text = slide_spec[\"title\"]\n",
    "\n",
    "            if slide_spec[\"type\"] == \"title\":\n",
    "                slide.placeholders[1].text = slide_spec.get(\"subtitle\",\"\")\n",
    "                continue\n",
    "\n",
    "            if slide_spec[\"type\"] == \"table\":\n",
    "                sql = slide_spec[\"sql\"]\n",
    "                table_json = run_sql_tool_factory(con)(sql)\n",
    "                data = json.loads(table_json)\n",
    "\n",
    "                if not data:\n",
    "                    continue\n",
    "\n",
    "                cols = list(data[0].keys())\n",
    "                rows = len(data)\n",
    "\n",
    "                table = slide.shapes.add_table(\n",
    "                    rows+1, len(cols),\n",
    "                    Inches(0.5), Inches(1.5),\n",
    "                    Inches(9), Inches(3)\n",
    "                ).table\n",
    "\n",
    "                for i,c in enumerate(cols):\n",
    "                    table.cell(0,i).text = c\n",
    "\n",
    "                for r in range(rows):\n",
    "                    for c in range(len(cols)):\n",
    "                        table.cell(r+1, c).text = str(data[r][cols[c]])\n",
    "\n",
    "            if slide_spec[\"type\"] == \"chart\":\n",
    "                sql = slide_spec[\"sql\"]\n",
    "                chart_data = run_sql_tool_factory(con)(sql)\n",
    "                chart_uri = create_chart_tool(chart_data, slide_spec[\"chart_type\"])\n",
    "\n",
    "                img_bytes = base64.b64decode(chart_uri.split(\",\")[1])\n",
    "                tmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".png\").name\n",
    "                with open(tmp,\"wb\") as f:\n",
    "                    f.write(img_bytes)\n",
    "\n",
    "                slide.shapes.add_picture(tmp, Inches(1), Inches(1.5), width=Inches(8))\n",
    "                os.remove(tmp)\n",
    "\n",
    "        filename = design.get(\"filename\",\"report.pptx\")\n",
    "        prs.save(filename)\n",
    "        return filename\n",
    "\n",
    "    return create_ppt\n",
    "\n",
    "def send_email_tool_factory(smtp_cfg):\n",
    "    def send_email(to_email, subject, body, attachment_path):\n",
    "        msg = EmailMessage()\n",
    "        msg[\"To\"] = to_email\n",
    "        msg[\"From\"] = smtp_cfg[\"user\"]\n",
    "        msg[\"Subject\"] = subject\n",
    "        msg.set_content(body)\n",
    "\n",
    "        with open(attachment_path,\"rb\") as f:\n",
    "            data = f.read()\n",
    "\n",
    "        msg.add_attachment(\n",
    "            data,\n",
    "            maintype=\"application\",\n",
    "            subtype=\"vnd.openxmlformats-officedocument.presentationml.presentation\",\n",
    "            filename=os.path.basename(attachment_path)\n",
    "        )\n",
    "\n",
    "        with smtplib.SMTP(smtp_cfg[\"host\"], smtp_cfg[\"port\"]) as s:\n",
    "            s.starttls()\n",
    "            s.login(smtp_cfg[\"user\"], smtp_cfg[\"password\"])\n",
    "            s.send_message(msg)\n",
    "\n",
    "        return f\"Email sent to {to_email}\"\n",
    "\n",
    "    return send_email\n",
    "\n",
    "def build_tools(con, smtp_cfg):\n",
    "    return [\n",
    "        Tool(name=\"run_sql\", func=run_sql_tool_factory(con), description=\"Run SQL on outages table\"),\n",
    "        Tool(name=\"create_chart\", func=create_chart_tool, description=\"Create chart from data\"),\n",
    "        Tool(name=\"create_ppt\", func=create_ppt_tool_factory(con), description=\"Generate PPT\"),\n",
    "        Tool(name=\"send_email\", func=send_email_tool_factory(smtp_cfg), description=\"Send email with attachment\")\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39a53a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app.py\n",
    "\n",
    "import streamlit as st\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "# ----------------- STREAMLIT CONFIG ------------------\n",
    "st.set_page_config(page_title=\"Outage Monitoring POC\", layout=\"wide\")\n",
    "st.title(\"üîå Outage Monitoring POC\")\n",
    "\n",
    "# ----------------- INIT DB & LLM ---------------------\n",
    "con = init_db(in_memory=True)\n",
    "llm = create_llm()\n",
    "\n",
    "SMTP_CFG = {\n",
    "    \"host\": \"smtp.gmail.com\",\n",
    "    \"port\": 587,\n",
    "    \"user\": \"your_email@gmail.com\",\n",
    "    \"password\": \"your_app_password\"\n",
    "}\n",
    "\n",
    "tools = build_tools(con, SMTP_CFG)\n",
    "# Create the agent\n",
    "agent = create_react_agent(llm, tools)\n",
    "\n",
    "# Create an executor to run it\n",
    "#agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "\n",
    "# ----------------- SIDEBAR ---------------------------\n",
    "mode = st.sidebar.radio(\"Mode\", [\"Chat\", \"Report Builder\", \"Admin\"])\n",
    "\n",
    "# ----------------- CHAT MODE -------------------------\n",
    "if mode == \"Chat\":\n",
    "    st.header(\"üí¨ Chat with Outage Assistant\")\n",
    "\n",
    "    if \"history\" not in st.session_state:\n",
    "        st.session_state.history = []\n",
    "\n",
    "    user_msg = st.chat_input(\"Ask something about outages‚Ä¶\")\n",
    "    if user_msg:\n",
    "        st.session_state.history.append((\"user\", user_msg))\n",
    "\n",
    "        with st.chat_message(\"user\"):\n",
    "            st.write(user_msg)\n",
    "\n",
    "        with st.chat_message(\"assistant\"):\n",
    "            resp = agent.invoke(user_msg)\n",
    "            st.write(resp)\n",
    "            st.session_state.history.append((\"assistant\", resp))\n",
    "\n",
    "    for role, msg in st.session_state.history:\n",
    "        with st.chat_message(role):\n",
    "            st.write(msg)\n",
    "\n",
    "# ----------------- REPORT BUILDER ---------------------\n",
    "elif mode == \"Report Builder\":\n",
    "    st.header(\"üìä Report Builder\")\n",
    "\n",
    "    prompt = st.text_area(\"Describe the report to generate\")\n",
    "\n",
    "    if st.button(\"Generate Report\"):\n",
    "        with st.spinner(\"Generating...\"):\n",
    "            resp = agent.invoke(prompt)\n",
    "\n",
    "        st.write(resp)\n",
    "\n",
    "        # Auto-detect PPT generated\n",
    "        ppts = [f for f in os.listdir(\".\") if f.endswith(\".pptx\")]\n",
    "        if ppts:\n",
    "            latest = sorted(ppts)[-1]\n",
    "            with open(latest,\"rb\") as f:\n",
    "                st.download_button(\"‚¨á Download PPT\", f, file_name=latest)\n",
    "\n",
    "# ----------------- ADMIN MODE ------------------------\n",
    "elif mode == \"Admin\":\n",
    "    st.header(\"üõ† Admin Mode\")\n",
    "\n",
    "    st.subheader(\"1. Load Outage Emails from Local Folder (POC)\")\n",
    "    folder = st.text_input(\"Email folder path\", \"./emails\")\n",
    "\n",
    "    if st.button(\"Parse Emails\"):\n",
    "        df = parse_emails_from_folder(folder, llm)\n",
    "        st.dataframe(df.head())\n",
    "        load_dataframe_to_duckdb(con, df)\n",
    "        st.success(\"Emails parsed and loaded into DuckDB!\")\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    st.subheader(\"2. ZIP Upload (coming soon)\")\n",
    "    st.info(\"This feature will be added later.\")\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    st.subheader(\"3. IMAP Email Ingestion (coming soon)\")\n",
    "    st.info(\"IMAP integration will be added in a future sprint.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cc5cae95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full folder path *********E:\\AI Utils\\Partner Performance Report\\Global_Hitec_Outage_Emails\n",
      "full file path *********E:\\AI Utils\\Partner Performance Report\\Global_Hitec_Outage_Emails\\outage_summary_001.txt\n",
      "üîç Parsing email file: outage_summary_001.txt ...\n",
      "full file path *********E:\\AI Utils\\Partner Performance Report\\Global_Hitec_Outage_Emails\\outage_summary_002.txt\n",
      "üîç Parsing email file: outage_summary_002.txt ...\n",
      "full file path *********E:\\AI Utils\\Partner Performance Report\\Global_Hitec_Outage_Emails\\outage_summary_003.txt\n",
      "üîç Parsing email file: outage_summary_003.txt ...\n",
      "full file path *********E:\\AI Utils\\Partner Performance Report\\Global_Hitec_Outage_Emails\\outage_summary_004.txt\n",
      "üîç Parsing email file: outage_summary_004.txt ...\n",
      "full file path *********E:\\AI Utils\\Partner Performance Report\\Global_Hitec_Outage_Emails\\outage_summary_005.txt\n",
      "üîç Parsing email file: outage_summary_005.txt ...\n",
      "‚úÖ Parsed 5 emails successfully.\n",
      "           partner_name outage_type               issue_details  \\\n",
      "0  TransGlobe Logistics   Unplanned  Unexpected storage failure   \n",
      "1           ExpressLine   Unplanned           Application crash   \n",
      "2             SwiftHaul     Planned     Infrastructure patching   \n",
      "3      MegaTrans Global     Planned  Scheduled hardware refresh   \n",
      "4             SwiftHaul   Unplanned   Firewall misconfiguration   \n",
      "\n",
      "  current_status                                    business_impact  \\\n",
      "0       Resolved           Limited impact on order acknowledgments.   \n",
      "1       Resolved  Order processing impacted. Premium(20), Regula...   \n",
      "2       Resolved     Order processing delayed for a short duration.   \n",
      "3       Resolved                       No business impact observed.   \n",
      "4       Resolved                       No business impact observed.   \n",
      "\n",
      "  manual_processing root_cause_available          outage_start_time  \\\n",
      "0          Required            Available  2021-12-19T00:51:00-08:00   \n",
      "1      Not Required        Not Available  2022-03-17T06:57:00-08:00   \n",
      "2      Not Required       Not Applicable  2020-12-01T11:32:00-08:00   \n",
      "3          Required                False       2025-05-30T02:12:00Z   \n",
      "4          Required            Available       2025-11-24T06:53:00Z   \n",
      "\n",
      "             outage_end_time  duration_hours resolution_details  \\\n",
      "0  2021-12-19T03:51:00-08:00               3               None   \n",
      "1  2022-03-17T10:57:00-08:00               4               None   \n",
      "2  2020-12-01T12:32:00-08:00               1               None   \n",
      "3       2025-05-30T06:12:00Z               4               None   \n",
      "4       2025-11-24T10:53:00Z               4               None   \n",
      "\n",
      "                                       email_subject email_date  \\\n",
      "0  Outage Summary - TransGlobe Logistics (Unplann...       None   \n",
      "1  Outage Summary - ExpressLine (Unplanned) - 202...       None   \n",
      "2  Outage Summary - SwiftHaul (Planned) - 2020-12-01       None   \n",
      "3  Outage Summary - MegaTrans Global (Planned) - ...       None   \n",
      "4  Outage Summary - SwiftHaul (Unplanned) - 2025-...       None   \n",
      "\n",
      "                    parsed_at             source_file  \n",
      "0  2025-11-19T01:09:23.260658  outage_summary_001.txt  \n",
      "1  2025-11-19T01:09:26.594475  outage_summary_002.txt  \n",
      "2  2025-11-19T01:09:29.945312  outage_summary_003.txt  \n",
      "3  2025-11-19T01:09:32.656182  outage_summary_004.txt  \n",
      "4  2025-11-19T01:09:35.802579  outage_summary_005.txt  \n",
      "           partner_name outage_type               issue_details  \\\n",
      "0  TransGlobe Logistics   Unplanned  Unexpected storage failure   \n",
      "1           ExpressLine   Unplanned           Application crash   \n",
      "2             SwiftHaul     Planned     Infrastructure patching   \n",
      "3      MegaTrans Global     Planned  Scheduled hardware refresh   \n",
      "4             SwiftHaul   Unplanned   Firewall misconfiguration   \n",
      "\n",
      "  current_status                                    business_impact  \\\n",
      "0       Resolved           Limited impact on order acknowledgments.   \n",
      "1       Resolved  Order processing impacted. Premium(20), Regula...   \n",
      "2       Resolved     Order processing delayed for a short duration.   \n",
      "3       Resolved                       No business impact observed.   \n",
      "4       Resolved                       No business impact observed.   \n",
      "\n",
      "  manual_processing root_cause_available          outage_start_time  \\\n",
      "0          Required            Available  2021-12-19T00:51:00-08:00   \n",
      "1      Not Required        Not Available  2022-03-17T06:57:00-08:00   \n",
      "2      Not Required       Not Applicable  2020-12-01T11:32:00-08:00   \n",
      "3          Required                False       2025-05-30T02:12:00Z   \n",
      "4          Required            Available       2025-11-24T06:53:00Z   \n",
      "\n",
      "             outage_end_time  duration_hours resolution_details  \\\n",
      "0  2021-12-19T03:51:00-08:00               3               None   \n",
      "1  2022-03-17T10:57:00-08:00               4               None   \n",
      "2  2020-12-01T12:32:00-08:00               1               None   \n",
      "3       2025-05-30T06:12:00Z               4               None   \n",
      "4       2025-11-24T10:53:00Z               4               None   \n",
      "\n",
      "                                       email_subject email_date  \\\n",
      "0  Outage Summary - TransGlobe Logistics (Unplann...       None   \n",
      "1  Outage Summary - ExpressLine (Unplanned) - 202...       None   \n",
      "2  Outage Summary - SwiftHaul (Planned) - 2020-12-01       None   \n",
      "3  Outage Summary - MegaTrans Global (Planned) - ...       None   \n",
      "4  Outage Summary - SwiftHaul (Unplanned) - 2025-...       None   \n",
      "\n",
      "                    parsed_at             source_file  \n",
      "0  2025-11-19T01:09:23.260658  outage_summary_001.txt  \n",
      "1  2025-11-19T01:09:26.594475  outage_summary_002.txt  \n",
      "2  2025-11-19T01:09:29.945312  outage_summary_003.txt  \n",
      "3  2025-11-19T01:09:32.656182  outage_summary_004.txt  \n",
      "4  2025-11-19T01:09:35.802579  outage_summary_005.txt  \n",
      "üíæ Using persistent DuckDB file (outages.duckdb).\n",
      "‚úÖ 5 records loaded into DuckDB successfully.\n",
      "***********Running test query on outages table:******************\n",
      "           partner_name outage_type               issue_details  \\\n",
      "0  TransGlobe Logistics   Unplanned  Unexpected storage failure   \n",
      "1           ExpressLine   Unplanned           Application crash   \n",
      "2             SwiftHaul     Planned     Infrastructure patching   \n",
      "3      MegaTrans Global     Planned  Scheduled hardware refresh   \n",
      "4             SwiftHaul   Unplanned   Firewall misconfiguration   \n",
      "5  TransGlobe Logistics   Unplanned  Unexpected storage failure   \n",
      "6           ExpressLine   Unplanned           Application crash   \n",
      "7             SwiftHaul     Planned     Infrastructure patching   \n",
      "8      MegaTrans Global     Planned  Scheduled hardware refresh   \n",
      "9             SwiftHaul   Unplanned   Firewall misconfiguration   \n",
      "\n",
      "  current_status                                    business_impact  \\\n",
      "0       Resolved           Limited impact on order acknowledgments.   \n",
      "1       Resolved  Order processing impacted. Premium(20), Regula...   \n",
      "2       Resolved     Order processing delayed for a short duration.   \n",
      "3       Resolved                       No business impact observed.   \n",
      "4       Resolved                       No business impact observed.   \n",
      "5       Resolved           Limited impact on order acknowledgments.   \n",
      "6       Resolved  Order processing impacted. Premium(20), Regula...   \n",
      "7       Resolved     Order processing delayed for a short duration.   \n",
      "8       Resolved                       No business impact observed.   \n",
      "9       Resolved                       No business impact observed.   \n",
      "\n",
      "  manual_processing root_cause_available   outage_start_time  \\\n",
      "0          Required            Available 2021-12-19 00:51:00   \n",
      "1      Not Required        Not Available 2022-03-17 06:57:00   \n",
      "2      Not Required       Not Applicable 2020-12-01 11:32:00   \n",
      "3          Required       Not Applicable 2025-05-30 02:12:00   \n",
      "4          Required            Available 2025-11-24 06:53:00   \n",
      "5          Required            Available 2021-12-19 00:51:00   \n",
      "6      Not Required        Not Available 2022-03-17 06:57:00   \n",
      "7      Not Required       Not Applicable 2020-12-01 11:32:00   \n",
      "8          Required                False 2025-05-30 02:12:00   \n",
      "9          Required            Available 2025-11-24 06:53:00   \n",
      "\n",
      "      outage_end_time  duration_hours resolution_details  \\\n",
      "0 2021-12-19 03:51:00             3.0               None   \n",
      "1 2022-03-17 10:57:00             4.0               None   \n",
      "2 2020-12-01 12:32:00             1.0               None   \n",
      "3 2025-05-30 06:12:00             4.0               None   \n",
      "4 2025-11-24 10:53:00             4.0               None   \n",
      "5 2021-12-19 03:51:00             3.0               None   \n",
      "6 2022-03-17 10:57:00             4.0               None   \n",
      "7 2020-12-01 12:32:00             1.0               None   \n",
      "8 2025-05-30 06:12:00             4.0               None   \n",
      "9 2025-11-24 10:53:00             4.0               None   \n",
      "\n",
      "                                       email_subject email_date  \\\n",
      "0  Outage Summary - TransGlobe Logistics (Unplann...        NaT   \n",
      "1  Outage Summary - ExpressLine (Unplanned) - 202...        NaT   \n",
      "2  Outage Summary - SwiftHaul (Planned) - 2020-12-01        NaT   \n",
      "3  Outage Summary - MegaTrans Global (Planned) - ...        NaT   \n",
      "4  Outage Summary - SwiftHaul (Unplanned) - 2025-...        NaT   \n",
      "5  Outage Summary - TransGlobe Logistics (Unplann...        NaT   \n",
      "6  Outage Summary - ExpressLine (Unplanned) - 202...        NaT   \n",
      "7  Outage Summary - SwiftHaul (Planned) - 2020-12-01        NaT   \n",
      "8  Outage Summary - MegaTrans Global (Planned) - ...        NaT   \n",
      "9  Outage Summary - SwiftHaul (Unplanned) - 2025-...        NaT   \n",
      "\n",
      "                   parsed_at             source_file  \n",
      "0 2025-11-19 01:06:37.732056  outage_summary_001.txt  \n",
      "1 2025-11-19 01:06:42.942043  outage_summary_002.txt  \n",
      "2 2025-11-19 01:06:48.777900  outage_summary_003.txt  \n",
      "3 2025-11-19 01:06:53.222405  outage_summary_004.txt  \n",
      "4 2025-11-19 01:06:56.734406  outage_summary_005.txt  \n",
      "5 2025-11-19 01:09:23.260658  outage_summary_001.txt  \n",
      "6 2025-11-19 01:09:26.594475  outage_summary_002.txt  \n",
      "7 2025-11-19 01:09:29.945312  outage_summary_003.txt  \n",
      "8 2025-11-19 01:09:32.656182  outage_summary_004.txt  \n",
      "9 2025-11-19 01:09:35.802579  outage_summary_005.txt  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Initialize the LLM\n",
    "llm = get_llm()\n",
    "\n",
    "# Step 2: Parse all emails from folder\n",
    "df = parse_emails_from_folder(HITEC_EMAIL_DIR, llm)\n",
    "print(df.head())\n",
    "\n",
    "# Step 3: Initialize database\n",
    "con = init_db(in_memory=False)\n",
    "\n",
    "# Step 4: Load parsed data into DuckDB\n",
    "load_dataframe_to_duckdb(con, df)\n",
    "\n",
    "# Step 5: Run a test SQL query\n",
    "print(\"***********Running test query on outages table:******************\")\n",
    "print(con.execute(\"SELECT * FROM outages\").df())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b797ee3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   count_star()\n",
      "0             0\n"
     ]
    }
   ],
   "source": [
    "print(con.execute(\"SELECT count(*) FROM outages\").df())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd4195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "\n",
    "# --- Init LLM and DB ---\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "con = init_db(in_memory=False)\n",
    "SMTP_CFG = {\n",
    "    \"host\": \"smtp.gmail.com\",\n",
    "    \"port\": 587,\n",
    "    \"user\": \"your_email@gmail.com\",\n",
    "    \"password\": \"your_app_password\"\n",
    "}\n",
    "tools = build_tools(con, SMTP_CFG)\n",
    "\n",
    "# --- ‚úÖ Bind tools to LLM so it can call them ---\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=llm_with_tools,\n",
    "    tools=tools,\n",
    "    system_prompt=(\n",
    "        \"You are OutageAnalysisAgent ‚Äî an expert in analyzing IT outage data stored in DuckDB.\\n\"\n",
    "        \"You have these tools:\\n\"\n",
    "        \"  - run_sql(sql): runs a SQL query and returns JSON records.\\n\"\n",
    "        \"  - create_chart(data_json, chart_type, x, y): plots data.\\n\"\n",
    "        \"  - create_ppt(design_json): creates presentation slides.\\n\\n\"\n",
    "        \"The outages table has these columns:\\n\"\n",
    "        \"  partner_name, outage_type, issue_details, current_status, business_impact,\\n\"\n",
    "        \"  manual_processing, root_cause_available, outage_start_time, outage_end_time,\\n\"\n",
    "        \"  duration_hours, resolution_details, email_subject, email_date, parsed_at, source_file.\\n\\n\"\n",
    "        \"Follow this process:\\n\"\n",
    "        \"1Ô∏è‚É£ When asked to show, count, or compare outage data ‚Äî call `run_sql()` with an explicit SQL query.\\n\"\n",
    "        \"2Ô∏è‚É£ Example: to get outages per partner in last month:\\n\"\n",
    "        \"   run_sql('SELECT partner_name, COUNT(*) AS total_outages FROM outages WHERE email_date >= CURRENT_DATE - INTERVAL 30 DAY GROUP BY partner_name')\\n\"\n",
    "        \"3Ô∏è‚É£ Then summarize the results in plain English.\\n\"\n",
    "        \"Do not ask clarifying questions; just use the tools and return the answer.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# --- Run test query ---\n",
    "response = agent.invoke({\"input\": \"Show total outages per partner for the last six month.\"})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d7f46a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# --- Helper functions -------------------------------------------------------\n",
    "\n",
    "def _try_run_limit0(con, sql):\n",
    "    \"\"\"Try running SQL with LIMIT 0 to check if it's valid in DuckDB.\"\"\"\n",
    "    if not sql.strip():\n",
    "        return False, \"Empty SQL\"\n",
    "    try:\n",
    "        test_sql = sql if re.search(r\"\\bLIMIT\\b\", sql, flags=re.IGNORECASE) else f\"{sql.rstrip(';')} LIMIT 0\"\n",
    "        con.execute(test_sql)\n",
    "        return True, None\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "\n",
    "def normalize_sql_to_duckdb(raw_sql: str, con, max_retries: int = 2) -> str:\n",
    "    \"\"\"\n",
    "    Uses the LLM to automatically translate arbitrary SQL into DuckDB-compatible SQL.\n",
    "    Falls back to safe regex replacements if validation fails.\n",
    "    \"\"\"\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    raw_sql = (raw_sql or \"\").strip()\n",
    "    if not raw_sql:\n",
    "        raise ValueError(\"Empty SQL provided to normalize_sql_to_duckdb\")\n",
    "\n",
    "    base_prompt = f\"\"\"\n",
    "    You are a SQL dialect translator. Convert this SQL into valid DuckDB SQL.\n",
    "    Return **only** the corrected SQL (no markdown, no code fences, no commentary).\n",
    "\n",
    "    If the SQL uses non-DuckDB syntax like DATEADD(), CONVERT(), or INTERVAL -6 MONTH,\n",
    "    rewrite it into DuckDB-compatible form (e.g., CURRENT_DATE - INTERVAL '6' MONTH).\n",
    "\n",
    "    SQL to fix:\n",
    "    {raw_sql}\n",
    "    \"\"\"\n",
    "\n",
    "    candidate_sql = raw_sql\n",
    "    for attempt in range(max_retries):\n",
    "        resp = llm.invoke(base_prompt).content.strip()\n",
    "        resp = re.sub(r\"```(?:sql)?\", \"\", resp, flags=re.IGNORECASE)\n",
    "        resp = resp.replace(\"```\", \"\").strip().rstrip(\";\")\n",
    "\n",
    "        ok, err = _try_run_limit0(con, resp)\n",
    "        if ok:\n",
    "            print(\"‚úÖ LLM produced valid DuckDB SQL.\")\n",
    "            return resp\n",
    "\n",
    "        # Retry: give the LLM the actual error\n",
    "        print(f\"‚ö†Ô∏è Attempt {attempt+1} failed: {err}\")\n",
    "        base_prompt = f\"\"\"\n",
    "        The following SQL failed in DuckDB with error:\n",
    "        {err}\n",
    "\n",
    "        Please fix the SQL so it works in DuckDB.\n",
    "        Return only the corrected SQL text (no markdown, no explanations).\n",
    "\n",
    "        Previous SQL:\n",
    "        {resp}\n",
    "        \"\"\"\n",
    "        candidate_sql = resp\n",
    "\n",
    "    # Final fallback ‚Äî regex-based sanitizer\n",
    "    print(\"‚öôÔ∏è Falling back to conservative regex fix...\")\n",
    "    fallback = re.sub(r\"```(?:sql)?\", \"\", raw_sql, flags=re.IGNORECASE)\n",
    "    fallback = re.sub(r\"```\", \"\", fallback).strip().rstrip(\";\")\n",
    "    fallback = re.sub(\n",
    "        r\"DATEADD\\s*\\(\\s*month\\s*,\\s*-(\\d+)\\s*,\\s*CURRENT_DATE\\s*\\)\",\n",
    "        r\"CURRENT_DATE - INTERVAL '\\1' MONTH\",\n",
    "        fallback,\n",
    "        flags=re.IGNORECASE,\n",
    "    )\n",
    "    fallback = re.sub(r\"\\bDATEADD\\s*\\(\", \"DATE_ADD(\", fallback, flags=re.IGNORECASE)\n",
    "    fallback = re.sub(r\"INTERVAL\\s*-\\s*(\\d+)\\s*MONTH\", r\"INTERVAL '\\1' MONTH\", fallback, flags=re.IGNORECASE)\n",
    "    ok, err = _try_run_limit0(con, fallback)\n",
    "    if ok:\n",
    "        print(\"‚úÖ Fallback regex produced valid SQL.\")\n",
    "        return fallback\n",
    "\n",
    "    raise RuntimeError(f\"‚ùå Could not fix SQL for DuckDB. Last error: {err}\\nCandidate: {candidate_sql}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4263586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sql(state: dict):\n",
    "    \"\"\"\n",
    "    Step 2 in the LangGraph workflow:\n",
    "    Takes a natural-language user query ‚Üí asks LLM to generate SQL ‚Üí normalizes to DuckDB.\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "\n",
    "    user_query = state.get(\"user_query\")\n",
    "    con = state.get(\"db_con\")\n",
    "\n",
    "    if not user_query:\n",
    "        raise KeyError(\"Missing 'user_query' in state\")\n",
    "    if not con:\n",
    "        raise KeyError(\"Missing 'db_con' in state\")\n",
    "\n",
    "    print(f\"üß† Generating SQL for user query: {user_query}\")\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    prompt = f\"\"\"\n",
    "    You are a data analyst generating SQL queries for a DuckDB database.\n",
    "\n",
    "    The table is named 'outages' with columns:\n",
    "    - partner_name, outage_type, issue_details, duration_hours, email_date,\n",
    "      outage_start_time, outage_end_time.\n",
    "\n",
    "    Generate a DuckDB-compatible SQL SELECT query that directly answers:\n",
    "    \"{user_query}\"\n",
    "\n",
    "    RULES:\n",
    "    - Return **only** the SQL text (no markdown, no commentary).\n",
    "    - **Do NOT add time filters (like date or interval)** unless the user's question\n",
    "      explicitly mentions time ranges such as \"last month\", \"last 6 months\", \"today\", etc.\n",
    "    - If no time range is mentioned, query the entire dataset.\n",
    "    - Use simple, standard DuckDB syntax.\n",
    "\n",
    "    Example:\n",
    "    ‚ùå Bad: Adds date filters when not asked.\n",
    "    ‚úÖ Good: SELECT partner_name, COUNT(*) AS total_outages FROM outages GROUP BY partner_name;\n",
    "    \"\"\"\n",
    "    raw_sql = llm.invoke(prompt).content.strip()\n",
    "    print(f\"üßÆ Raw SQL (before normalization):\\n{raw_sql}\")\n",
    "\n",
    "    clean_sql = normalize_sql_to_duckdb(raw_sql, con)\n",
    "    print(f\"üß© Final Clean SQL:\\n{clean_sql}\")\n",
    "\n",
    "    return {\"sql_query\": clean_sql, \"generated_at\": datetime.utcnow().isoformat()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "402997e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Outage Analytics Agent using LangGraph\n",
    "--------------------------------------\n",
    "\n",
    "‚úÖ Parses user queries (like \"show average outage duration per partner for last 6 months\")\n",
    "‚úÖ Generates SQL dynamically using LLM\n",
    "‚úÖ Runs SQL against DuckDB\n",
    "‚úÖ Creates charts + PPTs\n",
    "‚úÖ Provides natural summaries\n",
    "‚úÖ Auto-retries with relaxed SQL if query returns empty (self-healing)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import io\n",
    "import base64\n",
    "import tempfile\n",
    "import matplotlib.pyplot as plt\n",
    "import duckdb\n",
    "from pptx import Presentation\n",
    "from pptx.util import Inches\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Optional, Dict, Any\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1Ô∏è‚É£ Helper Functions\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def init_db(in_memory=False):\n",
    "    \"\"\"Initialize DuckDB database.\"\"\"\n",
    "    db_file = \":memory:\" if in_memory else \"outages.duckdb\"\n",
    "    con = duckdb.connect(database=db_file)\n",
    "    print(f\"üíæ Connected to {db_file}\")\n",
    "    con.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS outages (\n",
    "            partner_name VARCHAR,\n",
    "            outage_type VARCHAR,\n",
    "            issue_details VARCHAR,\n",
    "            current_status VARCHAR,\n",
    "            business_impact VARCHAR,\n",
    "            manual_processing VARCHAR,\n",
    "            root_cause_available VARCHAR,\n",
    "            outage_start_time TIMESTAMP,\n",
    "            outage_end_time TIMESTAMP,\n",
    "            duration_hours DOUBLE,\n",
    "            resolution_details VARCHAR,\n",
    "            email_subject VARCHAR,\n",
    "            email_date DATE,\n",
    "            parsed_at TIMESTAMP,\n",
    "            source_file VARCHAR\n",
    "        );\n",
    "    \"\"\")\n",
    "    return con\n",
    "\n",
    "\n",
    "def run_sql(con, sql: str):\n",
    "    \"\"\"Run SQL safely on DuckDB.\"\"\"\n",
    "    print(f\"Executing SQL: {sql}\")\n",
    "    try:\n",
    "        df = con.execute(sql).df()\n",
    "        print(f\"‚úÖ SQL returned {len(df)} rows\")\n",
    "        return df.to_json(orient=\"records\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå SQL Error: {e}\")\n",
    "        return json.dumps({\"error\": str(e)})\n",
    "\n",
    "\n",
    "def create_chart(data_json, chart_type=\"bar\", x=None, y=None):\n",
    "    \"\"\"Generate chart as base64 image.\"\"\"\n",
    "    data = json.loads(data_json)\n",
    "    if not data:\n",
    "        raise ValueError(\"Empty data for chart generation.\")\n",
    "\n",
    "    cols = list(data[0].keys())\n",
    "    x = x or cols[0]\n",
    "    y = y or cols[1]\n",
    "    xs = [str(r[x]) for r in data]\n",
    "    ys = [r[y] for r in data]\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    if chart_type == \"line\":\n",
    "        plt.plot(xs, ys, marker=\"o\")\n",
    "    else:\n",
    "        plt.bar(xs, ys)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format=\"png\")\n",
    "    buf.seek(0)\n",
    "    plt.close()\n",
    "    return \"data:image/png;base64,\" + base64.b64encode(buf.read()).decode()\n",
    "\n",
    "\n",
    "def create_ppt(data_json, filename=\"outage_summary.pptx\"):\n",
    "    \"\"\"Generate a simple PPT with table data.\"\"\"\n",
    "    data = json.loads(data_json)\n",
    "    prs = Presentation()\n",
    "    slide = prs.slides.add_slide(prs.slide_layouts[5])\n",
    "    slide.shapes.title.text = \"Outage Summary\"\n",
    "\n",
    "    if not data:\n",
    "        slide.placeholders[0].text = \"No outage data found.\"\n",
    "    else:\n",
    "        cols = list(data[0].keys())\n",
    "        rows = len(data)\n",
    "        table = slide.shapes.add_table(\n",
    "            rows+1, len(cols), Inches(0.5), Inches(1.5), Inches(9), Inches(3)\n",
    "        ).table\n",
    "        for i, c in enumerate(cols):\n",
    "            table.cell(0, i).text = c\n",
    "        for r in range(rows):\n",
    "            for c in range(len(cols)):\n",
    "                table.cell(r+1, c).text = str(data[r][cols[c]])\n",
    "\n",
    "    prs.save(filename)\n",
    "    return filename\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2Ô∏è‚É£ Universal Unwrapping + Helpers\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def unwrap_state(state):\n",
    "    \"\"\"Universal fix ‚Äî unpacks LangGraph‚Äôs nested state safely.\"\"\"\n",
    "    if isinstance(state, dict) and \"input\" in state and isinstance(state[\"input\"], dict):\n",
    "        print(\"ü™Ñ Auto-unwrapped LangGraph state.\")\n",
    "        merged = {**state, **state[\"input\"]}\n",
    "        merged.pop(\"input\", None)\n",
    "        return merged\n",
    "    return state\n",
    "\n",
    "\n",
    "def get_db_con(state):\n",
    "    \"\"\"Retrieve DB connection from state.\"\"\"\n",
    "    con = state.get(\"db_con\")\n",
    "    if not con:\n",
    "        raise ValueError(\"Missing database connection in state.\")\n",
    "    return con\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3Ô∏è‚É£ Agent State Definition\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "class AgentState(TypedDict, total=False):\n",
    "    user_query: str\n",
    "    sql_query: Optional[str]\n",
    "    data_json: Optional[str]\n",
    "    chart_uri: Optional[str]\n",
    "    ppt_path: Optional[str]\n",
    "    final_answer: Optional[str]\n",
    "    intent: Optional[Dict[str, Any]]\n",
    "    db_con: Any\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 4Ô∏è‚É£ Core Nodes\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def interpret_query(state: AgentState):\n",
    "    state = unwrap_state(state)\n",
    "    user_query = state.get(\"user_query\")\n",
    "    if not user_query:\n",
    "        raise KeyError(\"Missing 'user_query' in state input\")\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "    prompt = f\"\"\"\n",
    "    Determine the intent type for this user request and whether a chart or PPT is requested.\n",
    "    Return a JSON object exactly, e.g. {{ \"intent\": \"sql_query\", \"needs_chart\": true, \"notes\": \"...\" }}\n",
    "    Possible intents: sql_query, summary, report\n",
    "    User query: {user_query}\n",
    "    \"\"\"\n",
    "    resp = llm.invoke(prompt).content.strip()\n",
    "    try:\n",
    "        intent = json.loads(resp)\n",
    "    except Exception:\n",
    "        intent = {\"intent\": \"sql_query\", \"needs_chart\": (\"chart\" in user_query.lower()), \"notes\": \"\"}\n",
    "    print(f\"üß≠ Intent: {intent}\")\n",
    "    return {\"intent\": intent, \"user_query\": user_query}\n",
    "\n",
    "\n",
    "\n",
    "def execute_sql(state: AgentState):\n",
    "    state = unwrap_state(state)\n",
    "    sql_query = state.get(\"sql_query\")\n",
    "    con = get_db_con(state)\n",
    "\n",
    "    data_json = run_sql(con, sql_query)\n",
    "    return {\"data_json\": data_json}\n",
    "\n",
    "\n",
    "def retry_if_empty(state: AgentState):\n",
    "    state = unwrap_state(state)\n",
    "    data_json = state.get(\"data_json\")\n",
    "    sql_query = state.get(\"sql_query\")\n",
    "    con = get_db_con(state)\n",
    "\n",
    "    if not data_json or data_json.strip() in [\"\", \"[]\", \"null\", \"None\"]:\n",
    "        print(\"‚ö†Ô∏è No data returned, retrying with relaxed SQL...\")\n",
    "    else:\n",
    "        try:\n",
    "            data = json.loads(data_json)\n",
    "            if data and len(data) > 0:\n",
    "                print(\"‚úÖ Data present ‚Äî skipping retry.\")\n",
    "                return {\"data_json\": data_json, \"sql_query\": sql_query}\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    relaxed_sql = re.sub(\n",
    "        r\"WHERE\\s+email_date\\s*>=\\s*CURRENT_DATE\\s*-\\s*INTERVAL\\s+\\d+\\s+DAY\",\n",
    "        \"\",\n",
    "        sql_query,\n",
    "        flags=re.IGNORECASE,\n",
    "    )\n",
    "    relaxed_sql = re.sub(r\"WHERE\\s*(AND|OR)?\\s*$\", \"\", relaxed_sql, flags=re.IGNORECASE).strip()\n",
    "    print(f\"üîÅ Retrying with relaxed SQL:\\n{relaxed_sql}\")\n",
    "\n",
    "    data_json_retry = run_sql(con, relaxed_sql)\n",
    "    return {\"data_json\": data_json_retry, \"sql_query\": relaxed_sql}\n",
    "\n",
    "\n",
    "def maybe_chart_and_ppt(state: AgentState):\n",
    "    state = unwrap_state(state)\n",
    "    data_json = state.get(\"data_json\")\n",
    "    intent = state.get(\"intent\", {})\n",
    "    needs_chart = intent.get(\"needs_chart\", True)\n",
    "\n",
    "    chart_uri = ppt_path = None\n",
    "    if data_json and data_json.strip() not in [\"[]\", \"null\", \"None\"]:\n",
    "        if needs_chart:\n",
    "            chart_uri = create_chart(data_json)\n",
    "            print(\"üìà Chart generated.\")\n",
    "        ppt_path = create_ppt(data_json)\n",
    "        print(f\"üíæ PPT created: {ppt_path}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No data available for chart or PPT.\")\n",
    "\n",
    "    return {\"chart_uri\": chart_uri, \"ppt_path\": ppt_path}\n",
    "\n",
    "\n",
    "def summarize_results(state: AgentState):\n",
    "    state = unwrap_state(state)\n",
    "    data_json = state.get(\"data_json\")\n",
    "    user_query = state.get(\"user_query\")\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "    prompt = f\"\"\"\n",
    "    Summarize the following outage analytics result for this query:\n",
    "    {user_query}\n",
    "    Data: {data_json[:2000]}\n",
    "    Return a concise business-friendly summary.\n",
    "    \"\"\"\n",
    "    summary = llm.invoke(prompt).content.strip()\n",
    "    print(f\"üìù Summary: {summary}\")\n",
    "    return {\"final_answer\": summary}\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 5Ô∏è‚É£ Graph Definition\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def build_outage_agent_graph():\n",
    "    graph = StateGraph(AgentState)\n",
    "\n",
    "    graph.add_node(\"interpret\", interpret_query)\n",
    "    graph.add_node(\"generate_sql\", generate_sql)\n",
    "    graph.add_node(\"execute_sql\", execute_sql)\n",
    "    graph.add_node(\"retry_if_empty\", retry_if_empty)\n",
    "    graph.add_node(\"maybe_chart_and_ppt\", maybe_chart_and_ppt)\n",
    "    graph.add_node(\"summarize\", summarize_results)\n",
    "\n",
    "    graph.add_edge(\"interpret\", \"generate_sql\")\n",
    "    graph.add_edge(\"generate_sql\", \"execute_sql\")\n",
    "    graph.add_edge(\"execute_sql\", \"retry_if_empty\")\n",
    "    graph.add_edge(\"retry_if_empty\", \"maybe_chart_and_ppt\")\n",
    "    graph.add_edge(\"maybe_chart_and_ppt\", \"summarize\")\n",
    "\n",
    "    graph.set_entry_point(\"interpret\")\n",
    "    graph.set_finish_point(\"summarize\")\n",
    "    return graph.compile()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "21875da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Using persistent DuckDB file (outages.duckdb).\n",
      "\n",
      "üöÄ Running outage analytics agent...\n",
      "\n",
      "üß≠ Intent: {'intent': 'report', 'needs_chart': True, 'notes': 'The user is requesting a report on total outages, which typically involves data visualization.'}\n",
      "üß† Generating SQL for user query: Show total outages per partner \n",
      "üßÆ Raw SQL (before normalization):\n",
      "SELECT partner_name, COUNT(*) AS total_outages FROM outages GROUP BY partner_name;\n",
      "‚úÖ LLM produced valid DuckDB SQL.\n",
      "üß© Final Clean SQL:\n",
      "SELECT partner_name, COUNT(*) AS total_outages FROM outages GROUP BY partner_name\n",
      "Executing SQL: SELECT partner_name, COUNT(*) AS total_outages FROM outages GROUP BY partner_name\n",
      "‚úÖ SQL returned 4 rows\n",
      "‚úÖ Data present ‚Äî skipping retry.\n",
      "üìà Chart generated.\n",
      "üíæ PPT created: outage_summary.pptx\n",
      "üìù Summary: The outage analytics reveal the following total outages per partner:\n",
      "\n",
      "- **SwiftHaul** experienced the highest number of outages with a total of **4**.\n",
      "- **MegaTrans Global**, **TransGlobe Logistics**, and **ExpressLine** each reported **2** outages.\n",
      "\n",
      "This data highlights that SwiftHaul is facing more disruptions compared to the other partners.\n",
      "\n",
      "=== FINAL RESPONSE ===\n",
      "The outage analytics reveal the following total outages per partner:\n",
      "\n",
      "- **SwiftHaul** experienced the highest number of outages with a total of **4**.\n",
      "- **MegaTrans Global**, **TransGlobe Logistics**, and **ExpressLine** each reported **2** outages.\n",
      "\n",
      "This data highlights that SwiftHaul is facing more disruptions compared to the other partners.\n",
      "Chart URI (truncated): data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAyAAAAGQCAYAAABWJQQ0AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAASaZJREFUeJ\n",
      "PPT saved at: outage_summary.pptx\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# 6Ô∏è‚É£ Run Example\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    con = init_db(in_memory=False)\n",
    "    app = build_outage_agent_graph()\n",
    "\n",
    "    user_query = \"Show total outages per partner \"\n",
    "\n",
    "    print(\"\\nüöÄ Running outage analytics agent...\\n\")\n",
    "    response = app.invoke({\"user_query\": user_query, \"db_con\": con})\n",
    "\n",
    "    print(\"\\n=== FINAL RESPONSE ===\")\n",
    "    print(response.get(\"final_answer\") or \"No final answer generated.\")\n",
    "    if response.get(\"chart_uri\"):\n",
    "        print(\"Chart URI (truncated):\", response[\"chart_uri\"][:200])\n",
    "    if response.get(\"ppt_path\"):\n",
    "        print(\"PPT saved at:\", response[\"ppt_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9064c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "con = duckdb.connect(\"outages.duckdb\")\n",
    "print(\"Tables in DB:\")\n",
    "print(con.execute(\"SHOW TABLES\").fetchdf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3bebdac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   total_records\n",
      "0             10\n"
     ]
    }
   ],
   "source": [
    "print(con.execute(\"SELECT COUNT(*) AS total_records FROM outages\").fetchdf())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
